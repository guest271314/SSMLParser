# SpeechSynthesisSSMLParser
Implement SSML parsing for [Web Speech API](https://w3c.github.io/speech-api/speechapi.html)
----


The goal of this repository is to create free (libre) open source software algorithms to parse a well-formed SSML document in conformace with the specification for the purpose of uniform implementation at modern browsers.
----

[Speech Synthesis Markup Language (SSML) Version 1.1](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/) (SSML) parsing has not yet been implew-voice@w3.org](https://lists.w3.org/Archives/Public/www-voice/) [SSML parsing implementation at browsers](https://lists.w3.org/Archives/Public/www-voice/2017OctDec/0000.html).


[1.2 Speech Synthesis Process Steps](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#S1.2)

> 1. **XML parse:** An XML parser is used to extract the document tree and content from the incoming text document. The structure, tags and attributes obtained in this step influence each of the following steps.
> 
> 2. **Structure analysis:** The structure of a document influences the way in which a document should be read. For example, there are common speaking patterns associated with paragraphs and sentences.
> 
>  - Markup support: The [p](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_paragraph) and [s](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_sentence) elements defined in SSML explicitly indicate document structures that affect the speech output.
> - Non-markup behavior: In documents and parts of documents where these elements are not used, the [synthesis processor](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#term-processor) is responsible for inferring the structure by automated analysis of the text, often using punctuation and other language-specific data.
> 
> 3. **Text normalization:** All written languages have special constructs that require a conversion of the written form (orthographic form) into the spoken form. Text normalization is an automated process of the synthesis processor that performs this conversion. For example, for English, when "$200" appears in a document it may be spoken as "two hundred dollars". Similarly, "1/2" may be spoken as "half", "January second", "February first", "one of two" and so on. By the end of this step the text to be spoken has been converted completely into tokens. The exact details of what constitutes a token are language-specific. In English, tokens are usually separated by white space and are typically words. For languages with different tokenization behavior, the term "word" in this specification is intended to mean an appropriately comparable unit. Tokens in SSML cannot span markup tags except within the [token](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_token) and [w](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_word) elements. A simple English example is "cup<break/>board"; outside the token and w elements, the synthesis processor will treat this as the two tokens "cup" and "board" rather than as one token (word) with a pause in the middle. Breaking one token into multiple tokens this way will likely affect how the processor treats it.
> 
>  - Markup support: The say-as element can be used in the input document to explicitly indicate the presence and type of these constructs and to resolve ambiguities. The set of constructs that can be marked has not yet been defined but might include dates, times, numbers, acronyms, currency amounts and more. Note that many acronyms and abbreviations can be handled by the author via direct text replacement or by use of the [sub](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_sub) element, e.g. "BBC" can be written as "B B C" and "AAA" can be written as "triple A". These replacement written forms will likely be pronounced as one would want the original acronyms to be pronounced. In the case of Japanese text, if you have a synthesis processor that supports both Kanji and kana, you may be able to use the sub element to identify whether 今日は should be spoken as きょうは ("kyou wa" = "today") or こんにちは ("konnichiwa" = "hello").
> - Non-markup behavior: For text content that is not marked with the [say-as](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_say-as) element the synthesis processor is expected to make a reasonable effort to automatically locate and convert these constructs to a speakable form. Because of inherent ambiguities (such as the "1/2" example above) and because of the wide range of possible constructs in any language, this process may introduce errors in the speech output and may cause different processors to render the same document differently.
> 
> 4. **Text-to-phoneme conversion:** Once the synthesis processor has determined the set of tokens to be spoken, it must derive pronunciations for each token. Pronunciations may be conveniently described as sequences of phonemes, which are units of sound in a language that serve to distinguish one word from another. Each language (and sometimes each national or dialect variant of a language) has a specific phoneme set: e.g., most US English dialects have around 45 phonemes, Hawai'ian has between 12 and 18 (depending on who you ask), and some languages have more than 100! This conversion is made complex by a number of issues. One issue is that there are differences between written and spoken forms of a language, and these differences can lead to indeterminacy or ambiguity in the pronunciation of written words. For example, compared with their spoken form, words in Hebrew and Arabic are usually written with no vowels, or only a few vowels specified. In many languages the same written word may have many spoken forms. For example, in English, "read" may be spoken as "reed" (I will read the book) or "red" (I have read the book). Both human speakers and synthesis processors can pronounce these words correctly in context but may have difficulty without context (see "Non-markup behavior" below). Another issue is the handling of words with non-standard spellings or pronunciations. For example, an English synthesis processor will often have trouble determining how to speak some non-English-origin names, e.g. "Caius College" (pronounced "keys college") and President Tito (pronounced "sutto"), the president of the Republic of Kiribati (pronounced "kiribass").
> 
> - Markup support: The [phoneme](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_phoneme) element allows a phonemic sequence to be provided for any token or token sequence. This provides the content creator with explicit control over pronunciations. The say-as element might also be used to indicate that text is a proper name that may allow a synthesis processor to apply special rules to determine a pronunciation. The [lexicon](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_lexicon) and [lookup](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_lookup) elements can be used to reference external definitions of pronunciations. These elements can be particularly useful for acronyms and abbreviations that the processor is unable to resolve via its own text normalization and that are not addressable via direct text substitution or the sub element (see paragraph 3, above).
> - Non-markup behavior: In the absence of a phoneme element the synthesis processor must apply automated capabilities to determine pronunciations. This is typically achieved by looking up tokens in a pronunciation dictionary (which may be language-dependent) and applying rules to determine other pronunciations. Synthesis processors are designed to perform text-to-phoneme conversions so most words of most documents can be handled automatically. As an alternative to relying upon the processor, authors may choose to perform some conversions themselves prior to encoding in SSML. Written words with indeterminate or ambiguous pronunciations could be replaced by words with an unambiguous pronunciation; for example, in the case of "read", "I will reed the book". Authors should be aware, however, that the resulting SSML document may not be optimal for visual display.
> 
> 5. **Prosody analysis:** Prosody is the set of features of speech output that includes the pitch (also called intonation or melody), the timing (or rhythm), the pausing, the speaking rate, the emphasis on words and many other features. Producing human-like prosody is important for making speech sound natural and for correctly conveying the meaning of spoken language.
> 
> - Markup support: The [emphasis](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_emphasis) element, [break](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_break) element and [prosody](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_prosody) element may all be used by document creators to guide the synthesis processor in generating appropriate prosodic features in the speech output.
> - Non-markup behavior: In the absence of these elements, synthesis processors are expert (but not perfect) in automatically generating suitable prosody. This is achieved through analysis of the document structure, sentence syntax, and other information that can be inferred from the text input.
> While most of the elements of SSML can be considered high-level in that they provide either content to be spoken or logical descriptions of style, the break and prosody elements mentioned above operate at a later point in the process and thus must coexist both with uses of the emphasis element and with the processor's own determinations of prosodic behavior. Unless specified in the appropriate sections, details of the interactions between the processor's own determinations and those provided by the author at this level are processor-specific. Authors are encouraged not to casually or arbitrarily mix these two levels of control.
> 
> 6. **Waveform production:** The phonemes and prosodic information are used by the synthesis processor in the production of the audio waveform. There are many approaches to this processing step so there may be considerable processor-specific variation.
> 
> - Markup support: The [voice](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_voice) element allows the document creator to request a particular voice or specific voice qualities (e.g. a young male voice). The [audio](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_audio) element allows for insertion of recorded audio data into the output stream, with optional control over the duration, sound level and playback speed of the recording. Rendering can be restricted to a subset of the document by using the trimming attributes on the [speak](https://www.w3.org/TR/2010/REC-speech-synthesis11-20100907/#edef_speak) element.
> - Non-markup behavior: The default volume/sound level, speed, and pitch/frequency of both voices and recorded audio in the document are that of the unmodified waveforms, whether they be voices or recordings.

See also

> [5.2.3 SpeechSynthesisUtterance Attributes](https://w3c.github.io/speech-api/speechapi.html#utterance-attributes)
> **_text_ attribute**
> This attribute specifies the text to be synthesized and spoken for this utterance. This may be either plain text or a complete, well-formed SSML document. [[SSML](https://w3c.github.io/speech-api/speechapi.html#ref-ssml)] For speech synthesis engines that do not support SSML, or only support certain tags, the user agent or speech engine must strip away the tags they do not support and speak the text. There may be a maximum length of the text, it may be limited to 32,767 characters.

